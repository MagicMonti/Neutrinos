\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[german]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[backend=bibtex,bibencoding=ascii,style=numeric,sorting=none]{biblatex}
\addbibresource{source.bib}


\author{Julian Köberle}
\title{Notizen zu Fermionen im gekrümmten Raum}
\begin{document}
\maketitle

\newcommand{\R}{\mathbb{R}}
\newcommand{\x}{\textbf{x}}
\newcommand{\y}{\textbf{y}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\at}{\textbf{a}^\top}
\newcommand{\n}{\textbf{n}^\top_\theta}
\newcommand{\A}{A_i^{\ j}}

\begin{abstract}
Das sind Notizen zu meinen zu der Arbeit Fermionen im gekrümmten Raum. Diese Notizen können etwaige Rechtschreibfehler, Grammatikfehler und stilistische Unfeinheiten beinhalten und sind deshalb nicht für die Weitergabe an Dritte bestimmt.
\end{abstract}

\tableofcontents 

\section{The Problem}
Let's suppose one is given a set of data-points $\{ \x_1, \x_2, \ldots , \x_n\}$ on a circle with radius $r$. Further we will assume that those points are generated by  underlying function $F$, such that, $F$ produces the change of $\x_i$ with respect to some parameter for example time. The exact structure of $F$ is unknown.
The goal is to find a function $F'$ such that $F'$ approximates $F$ , to put it more mathematically:
\begin{equation}
\begin{split}
\label{neualODE}
\frac{d\x}{dt} = F(\x(t),t) \approx F'(\x(t), t, \theta),
\\
\x : \R \mapsto \{\y \in \R^2 | \y^\top \y = r^2\},
\end{split}
\end{equation}where $F'$ is a neural-network with its corresponding network parameters $\theta$ as we will see in the following sections, an algorithm to solve (\ref{neualODE}) is independent of $r$, so we can set $r=1$ without loss of generality.

To enforce the $S^1$ constraint $\forall t$ one makes the ansatz $F' = G\x$ such that $G$ is a antisymmetric matrix.
\begin{proof} We want to preserve the norm of $\x$,
$$
\x^\top \x = const
\Rightarrow
\frac{d \x^\top \x}{dt} = \dot{\x}^\top \x + \x^\top \dot{\x} = 0.
$$
Inserting $\frac{d \x }{dt} = G \x$ gives,
$$
\x^\top G^\top \x + \x^\top G\x = 0.
$$
This needs to hold for all $\x \ \Rightarrow G^\top = - G $  
\end{proof} 
Because in $\R^2 \ G$ has only one\footnote{in $\R^D \ G$ has $\frac{(D-1)D}{2}$ free parameters} free parameter, it is useful to replace $G \rightarrow A g$,
where $A = \begin{pmatrix}
0 & 1 \\
-1 & 0
\end{pmatrix}$ 
and $g$ is a scalar quantity. Hence equation (\ref{neualODE}) can be written as,
\begin{equation}
\label{neualODE_2}
\frac{d\x(t)}{dt} = g(\x(t),t, \theta)A \x(t).
\end{equation}

\section{Forward process}
To solve equation (\ref{neualODE_2}), an exponential integrator is useful, since it enforces the $S^1$ constraint for every time step. In most cases it is sufficient to solve equation (\ref{neualODE}) with a method which is of order one in time, if the time step $\Delta t$ was chosen sufficiently small. 
Let $\x_0 = \x(t)$, $\x_1 = \x(t+\Delta t) $ and $g_0 = g(\x(t), t)$,
\begin{equation}
\label{euler}
\x_1 = e^{\Delta t Ag_0}\x_0,
\end{equation}
$\Rightarrow$ 
\begin{equation}
\label{forward}
\x_n = e^{\Delta t Ag_{n-1}}e^{\Delta t Ag_{n-2}}\ldots e^{\Delta t Ag_{0}}\x_0,
\end{equation}
where $\x_n = \x(t+T)$,
it is easy to see that (\ref{euler}) implements the $S^1$ constraint $\forall t$.
\begin{proof}
$$
\x_1^\top \x_1 = \x_0^\top e^{\Delta t g_0 A^\top}e^{\Delta t Ag_0}\x_0 \stackrel{(A^\top = -A)}{=} e^{-\Delta t g_0 A}e^{\Delta t Ag_0}\x_0 \stackrel{([A,A] =0)}{=} \x_0^\top \x_0.
$$
\end{proof}

In the following and in the sections thereafter, we switch to index notation such that quantities are summed over repeated indices.
$$
\x \rightarrow x^a_i = \begin{pmatrix}
(x^1_1, x^1_2)  \\
(x^2_1, x^2_2) \\
\vdots  \\
(x^{bs}_1,x^{bs}_2)
\end{pmatrix}
,$$where $bs$ means batch size.
The indices $a,b,c \ldots$ are used to notate batch components. Indices like $i,j,k \ldots$ are used to notate vector components.
$$
A \rightarrow \A,
$$
$$
g \rightarrow \text{diag}(\underbrace{g^1,g^2,\ldots g^{bs}}_ {g^a}) := g^a_b.
$$
Note, instead of $g^a_b x_i^b$ one could write $(g \ast x_i)^a $ where $\ast$ is element-wise multiplication, but we prefer the first one, so it won't be an abuse of notation.


\section{Backward process}



Let's define a continuous loss function $\Lcal$ which takes two parameters: the real value of $x^a_i$ solved by $F$ and the prediction given by the Neural-ode $\hat{x^a_i}$ solved by $F'$ both evaluated at $t$.
\begin{equation}
\label{loss_function}
\begin{split}
\Lcal : \mathbb{R}^{2\times bs}\times \mathbb{R}^{2\times bs} \mapsto \mathbb{R}, \\
\Lcal = \Lcal(x^a_i,\hat{x^a_i}),
\end{split}
\end{equation}

where again $bs$ means batch-size. In the following we leave $\Lcal$ arbitrary and is only constrained by (\ref{loss_function}).
The overall goal is to find network parameters $\theta$ which minimizes the loss. This can be formulated as,
\begin{equation}
\theta_{min} = \text{argmin}_\theta \ \mathcal{L},
\end{equation}
since $\mathcal{L}$ is at least in $C^1$, one needs to find the solution of 
${d\Lcal}/{d\theta} = 0$, this is mostly done with Stochastic Gradient Descent (SGD) or Adam \cite{ADAM}. 
From eq. (\ref{forward}) one can easily see that back propagation is very memory and time-consuming, since the whole computational graph needs to be stored. A more sophisticated method is used to get ${d \Lcal}/{d\theta}$.
We introduce the adjoint sensitivity method, whose equation can be derived by introducing Lagrange multipliers. The benefit of this method is that only a fraction of the graph needs to be stored. In the following time-dependencies are suppressed and $x^a_i$ is used instead of $\hat{x^a_i}$ to not overload notation. This is motivated by the fact that $x^a_i$ is not dependent $\theta$ but so is $\hat{x^a_i}$. Since we are looking for ${d \Lcal}/{d\theta}$ derivatives of $x^a_i$ do not come into play, so the switch of notation is justified.

Let's define a scalar quantity,

\begin{equation}
K = \int^{T}_0 dt\ b^i_a (\dot{x}^a_i - g^a_b \A x_j^b).
\end{equation}
$b^i_a = b^i_a(t)$ are the Lagrange multipliers. The quantity $K$ is always zero and therefore one can write:
\begin{equation}
\label{dl}
\frac{d\Lcal}{d\theta_l} = \frac{d\Lcal}{d\theta_l}  + \frac{d K}{d\theta_l}.
\end{equation}
By integration by parts, $K$ can be written as,
$$
K = b^i_a x^a_i |^T_{0} - \int^T_0 dt\ \left( \dot{b}^i_a x^a_i + g^a_b \ b^i_a \A x^b_j \right) .
$$
Taking the derivative one gets,
$$
\frac{d K }{d\theta_l} = b^i_a \frac{dx^a_i}{d\theta_l} |^T_0 - \int^T_0  dt \left( \dot{b}^i_a \frac{dx^a_i}{d\theta_l} + \frac{dg^a_b}{d\theta_l} \ b^i_a \A x^b_j + g^a_b \ b^i_a \A \frac{dx^b_j}{d\theta_l}\right).
$$
It is worth noting that $x^a_j(t=0)$ does not depend on $\theta$, only $x^a_j(t=T)$ does.

Inserting the following relation

\begin{equation}
\label{dg}
\frac{dg^a_b}{d\theta_l} = \frac{\partial g^a_b}{\partial x^c_i} \frac{d x^c_i}{d\theta_l} + \frac{\partial g^a_b}{\partial \theta_l}
\end{equation} one gets,

\begin{equation}
\label{dK}
\frac{d K }{d\theta_l} = b^i_a \frac{d x^a_i}{d\theta_l} |^T_0 - \int^T_0 dt\ \left( \dot{b}^i_a \frac{d x^a_i}{d\theta_l} + \left(\frac{\partial g^a_b}{\partial x^c_k} \frac{d x^c_k}{d\theta_l} + \frac{\partial g^a_b}{\partial \theta_l} \right) \ b^i_a \A x^b_j + g^a_b \ b^i_a \A \frac{d x^b_j}{d\theta_l} \right)
\end{equation} collecting terms which depend on $\frac{d x^a_i}{d\theta_l}$, equation (\ref{dK}) reads

$$
\frac{d K }{d\theta_l} = b^i_a \frac{d x^a_i}{d\theta_l} |^T_0 - \int^T_0 dt\ \left(  \left( \dot{b}^i_a  + g^c_a \ b^n_c A_n^{\ i}  + \frac{\partial g^d_b}{\partial x^a_i} b^m_d A_m^{\ j} x^b_j \right) \frac{d x^a_i}{d\theta_l} + \frac{\partial g^a_b}{\partial \theta_l}  \ b^i_a \A x^b_j \right)
$$ using equation (\ref{dl}) and 
$$
\frac{d\Lcal}{d\theta_l} = \frac{\partial \Lcal}{\partial x^a_i} \frac{d x^a_i}{d\theta_l} 
$$by using $b^i_a(0)= 0$ one obtains,
\begin{equation}
\label{dLdtheta}
\begin{aligned}
& \frac{d\Lcal}{d\theta_l}|^{T} = \left(\frac{\partial \Lcal}{\partial x^a_i}  + b^i_a  \right) \frac{d x^a_i}{d\theta_l} |^T - \\
&\int^T_0 dt\ (\dot{b}^i_a  + g^c_a \ b^n_c A_n^{\ i}  + \frac{\partial g^d_b}{\partial x^a_i} b^m_d A_m^{\ j} x^b_j) \frac{d x^a_i}{d\theta_l}+ \frac{\partial g^a_b}{\partial \theta_l} b^i_a \A x^b_j
\end{aligned}
\end{equation} where $|^{T}$ means evaluated at time $t=T$. Since eq. (\ref{dLdtheta}) needs to hold for all ${d x^a_i}/{d\theta_l}$ follows,

\begin{equation}
\label{ad1_}
\frac{\partial \Lcal}{\partial x^a_i} |^T= -  b^i_a  |^T,
\end{equation}
\begin{equation}
\label{ad2_}
\dot{b}^i_a  + g^c_a \ b^n_c A_n^{\ i}  + \frac{\partial g^d_b}{\partial x^a_i} b^m_d A_m^{\ j} x^b_j=0, 
\end{equation}
\begin{equation}
	\label{ad3_}
	\frac{d\Lcal}{d\theta_l}|^{T}  = - \int^T_0 dt\ \frac{\partial g^a_b}{\partial \theta_l} b^i_a \A x^b_j, 
\end{equation}
Multiplying eq. (\ref{ad2_}) with $A_i^{\ j} x^b_j$
follows, $$\dot{b}^i_a A_i^{\ j} x^b_j - g^c_a \ b^j_c  x^b_j  + \frac{\partial g^d_e}{\partial x^a_i} b^m_d A_m^{\ j} x^e_j A_i^{\ k} x^b_k=0.$$
Using the substitution $P^{\ b}_a(x(t=T)) := b^i_a A_i^{\ j} x^b_j$
this equation can be written as, 
$$
\frac{d P^{\ b}_a}{dt} +  \frac{\partial g^d_e}{\partial x^a_i}P^{\ e}_d A_i^{\ k} x^b_k=0.
$$


Since $g^a_b$ has only diagonal entries, the expression ${\partial g^a_b}/{\partial \theta_l} \  b^i_a \A x^b_j$ in eq. (\ref{ad3_}) reduces to ${\partial g^a}/{\partial \theta_l} \ P^{\text{diag} }_a$, where $P^{\text{diag} }_a$ are the diagonal components of $P^a_b$. and $g^a$ are the diagonal compoents of $g^a_b$. 
\begin{proof}
	by introducing 
	\[ \delta_{ba}^c = \begin{cases} 
		1 & a=b=c \\
		0 & \textrm{else}
	\end{cases}
	\]
	
	$P^{\text{diag} }$ can be written in index notation as  $P^{\ b}_c \delta_{ba}^c$, likewise $g^a = g^b_c \delta_{b}^{ac}$ 
	
	$g^b_c = \text{diag}(g^a)^b_c = g^a \delta^b_{ac} \ \Rightarrow g^b_c P_b^{\ c} = g^a \delta^b_{ac} P_b^{\ c} = g^a P^{\text{diag} }_a$ 
	
\end{proof}


Therefore, we are only interested in the diagonal entries of $P$, hence equation (\ref{ad2_}) can be written as,
\begin{equation}
\label{adPdiag}
\frac{d P^{\text{diag} }_e}{dt} +  \frac{\partial g^d}{\partial x^a_i}P^{\text{diag} }_d A_i^{\ k} x^b_k \delta^a_{be}=0,
\end{equation}
by solving eq. (\ref{adPdiag}) numerically, one needs to make sure that the indices are contracted efficient, since  $\delta_{ba}^c$ carries a lot of entries, but most of them are zero anyway.
With the above defined substitution equation (\ref{ad3_}) reads,
\begin{equation}
\frac{d\Lcal}{d\theta_l}|^{T}  = - \int^T_0 dt\ \frac{\partial g^a}{\partial \theta_l} P^{\text{diag} }_a
\end{equation}

\section{Data Preprocessing}
\begin{figure}[h]
    \centering
    \includegraphics{output2.png}
    \caption{The solution to equation (\ref{dphi}) is shown in blue, from this the Cartesian $x$ and $y$ components are extracted,  shown in green and orange}
    \label{fig:fig1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics{output.png}
    \caption{Randomly selected data points $(x_i, y_i)$ are shown in blue and green. Those points define the selection of the final point $(x_f, y_f)=((x(t_i+t_p), y(t_i+t_p))$ shown in orange and red, where $t_p$ is the propagation time.}
    \label{fig:fig2}
\end{figure}

To test the above algorithm, a function $F$ was chosen such that equation (\ref{neualODE}) does \textbf{not} describe a linear differential equation, otherwise the network would drop to a single layer. Further $F$ was chosen to be smooth and not too complicated in an informal sense, additionally it is worth assuming $F$ is bounded, since going from polar coordinates to Cartesian coordinates discontinuities could occur. An obvious chose could be $F(\phi) = \text{sin}(\phi)$ in polar coordinates. This enforces the $S^1$ constraint. From the solution of 
\begin{equation}
	\label{dphi}
	\frac{d \phi}{dt} = \text{sin}(\phi),
\end{equation}$x(t)=\text{arccos}(\phi(t))$ and $y(t)=\text{arcsin}(\phi(t))$ components are extracted. A propagation-time is defined which correlates the starting point $\x_i$ to the final point $\x_f$, as shown in figure (\ref{fig:fig2}). 
$$
(x^a_i)_\text{init} =  \begin{pmatrix}
x_0 & y_0 \\
x_1 & y_2 \\
\vdots & \vdots \\
x_{bs} & y_{bs} 
\end{pmatrix}_\text{init} \xrightarrow[]{F'} (\hat{x}^a_i)_\text{final} = \begin{pmatrix}
\hat{x}_0 & \hat{y}_0 \\
\hat{x}_1 & \hat{y}_2 \\
\vdots & \vdots \\
\hat{x}_{bs} & \hat{y}_{bs} 
\end{pmatrix}_\text{final} \approx \begin{pmatrix}
x_0 & y_0 \\
x_1 & y_2 \\
\vdots & \vdots \\
x_{bs} & y_{bs} 
\end{pmatrix}_\text{final}
$$
\section{Experiments}
The experiments shows that the adjoint sensitivity method solves the $S^1$ toy problem perfectly, with known and unknown starting conditions. To even get better results one can increase the batch-size, the sample-size, in other words increase the data set, increase the network parameters and increase the number of training epochs. The loss function was chosen to be the usual $L_2$-norm averaged over the whole batch. A very interesting hyper-parameter is the propagation-time, which specifies indirectly which method to choose. For a data set where the propagation-time needs to be relatively large, the adjoint sensitivity method dominates over common back propagation method, since the computational-graph would become insanely large. For short very small propagation-time the common back propagation method is to be preferred, since the adjoint-sensitivity method carries a big overhead. In this toy problem the propagation time could be chosen to be relatively small, therefore there is no real speedup to be expected. 

\begin{figure}[h]
    \centering
    \includegraphics{output3.png}
    \caption{Prediction versus the real values of the $x$ and $y$ components. The prediction was made on the same starting condition $\phi(t=0) = \pi/2$ which was chosen on the data-set. Calculations show that $x^2+y^2=1$ is conserved for all time.}
    \label{fig:fig3}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics{output4.png}
    \caption{Prediction versus the real values of the $x$ and $y$ components. The prediction was made on a different starting condition $\phi(t=0) = \pi/4$ and different radius $R=2$. Calculations show that $x^2+y^2=4$ is conserved for all time}
    \label{fig:fig4}
\end{figure}


\section{Generalization}
Equation (\ref{neualODE}) can be easily generalized to $\mathbb{R}^D$ with $D\in \mathbb{N}$. Such that 
$$
\frac{d x^i_a}{dt} = x^j_b G^{bi}_{aj}
$$
Where $G^{bi}_{aj}$ is diagonal in $a, b$ and antisymmetric in $i,j$ to enforce the conserved norm on $\x^i_a$. $a,b = 1, \ldots ,bs$ and $i,j= 1,2\ldots,D$. 

$$
G^{bi}_{aj} = \begin{pmatrix}
G^{1i}_{1j} & 0 & \ldots & 0 \\
0 & G^{2i}_{2j} & \ldots  & 0 \\
\vdots & \vdots \\
0 & 0 & \ldots  & G^{bs\ i}_{bs\ j}
\end{pmatrix}
$$
$G^{bi}_{aj}$ is as written here is an enormously large object, but has only $bs \times (D-1)D/2$ free parameters. Since $G^{bi}_{aj}$ is supposed to be our neural network $f$.
$f$ has therefore $(D-1)D/2$ output neurons with a corresponding output shape $[bs,(D-1)D/2]$. 
Defining the mapping $M:\mathbb{R}^{bs \times (D-1)D/2} \mapsto  \mathbb{R}^{bs \times bs \times D \times D }$, such that $M$ is surjective helps to derive the equations of the adjoint sensitivity method, which are analogue to those defined above in the $S^1$, but instead of $gA$ using $G$.

\break
Another generalization to $SU(N)$ can be achieved by writing,
\begin{equation}
\label{SUN}
	\frac{d U^{ik}_a}{dt} = i \ U^{ij}_b G^{bk}_{aj}
\end{equation}

where $(U_a)\in SU(N) \rightarrow U_a \in \mathbb{C}^{N\times N}$ in the fundamental representation, and $(G_a) \in \mathfrak{su}(N), G^{ai}_{bj} := diag((G_1)^i_j, (G_2)^i_j, \ldots, (G_{bs})^i_j) | G_a \in \mathbb{C}^{N\times N}$, where we have used physicist convention where the generators are hermitian. Since $G$ has $N^2-1$ free parameters it is convenient to implement an bijective mapping $M:\mathbb{R}^{bs \times N^2-1 } \mapsto \mathbb{C}^{bs \times bs \times N \times N }$ similar to the $\mathbb{R}^D$ case above.

\begin{proof}
 In the following indices $i,j$ are suppressed. Without loss of generality, we concentrate on batch-size of order $1$, therefore indices $a,b$ can also be neglected. To see $U(t)$ stays in $SU(N)$ one can solve the above differential equation (\ref{SUN}) and gets,
$$
U(t) =  \underbrace{U(0)}_{\in SU(N)} \underbrace{\mathcal{T}\text{exp}(i\int_0^t dt' G(U(t'),t))}_{\in SU(N)}, $$
follows $ U(t) \in SU(N)\  \forall t $
where $\mathcal{T}$ is the time-ordering-operator.
\end{proof}

\section{Conclusion}
Although there was no real speedup in this $S^1$ toy problem it describes a good foundation to general cases like $SU(N)$ which is a crucial part in high energy physics to calculate Wilson-flows. 

\printbibliography


\end{document}

